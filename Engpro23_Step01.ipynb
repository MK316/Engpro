{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyPPROgwi8gLvpDE6rbOdHDy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MK316/Engpro/blob/main/Engpro23_Step01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ’› Data (23. 6. 3)\n",
        "\n",
        "+ Engpro Spring 23 (N=26, F=21, M=5)\n",
        "+ **Rainbow passage**\n",
        "\n",
        "## Data process\n",
        "+ foramt to .wav\n",
        "+ stereo to mono\n",
        "+ heading removed\n",
        "+ saved in Google asrdata as a zip: Engpro23PPre.zip"
      ],
      "metadata": {
        "id": "5rlcomSRlGXe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [1] Info file to read and data summary\n",
        "+ File from Github\n",
        "+ Engpro23_info.csv: 26 files with info\n",
        "\n",
        "|Index|Subject|Gender\n",
        "|--|--|--|\n",
        "|1|F01|F|"
      ],
      "metadata": {
        "id": "txu8Xkdpq-oz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "url = \"https://github.com/MK316/Engpro/raw/main/data/Engpro23_info.csv\"\n",
        "df = pd.read_csv(url)\n",
        "df.tail()"
      ],
      "metadata": {
        "id": "iW-bk7a-rWRb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Rainbow passage\n",
        "[Github link](https://raw.githubusercontent.com/MK316/Engpro/main/data/rainbow.txt)"
      ],
      "metadata": {
        "id": "uv5jX63P0hhv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "url = \"https://raw.githubusercontent.com/MK316/Engpro/main/data/rainbow.txt\"\n",
        "response = requests.get(url)\n",
        "rainbow = response.text\n",
        "rainbow\n"
      ],
      "metadata": {
        "id": "IK_5baBSZu4P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/MK316/Engpro/main/data/rainbow_sents.csv\"\n",
        "sents = pd.read_csv(url)\n",
        "sents.head()"
      ],
      "metadata": {
        "id": "vIq25Ili0f70"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [2] Reading audio files from Google Drive (zip file)"
      ],
      "metadata": {
        "id": "Enw32YlPuMtI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [2A] Audio files from Google to Colab 'myaudio' directory"
      ],
      "metadata": {
        "id": "UuD_UqQ20_94"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#@markdown ðŸŽ¯Mount Google drive and list files (e.g., \"asrdata\" folder in my case)\n",
        "from google.colab import drive \n",
        "import os\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "mydir = input(\"Type the file directory in your google drive: e.g., asrdata  \")\n",
        "!ls \"/content/drive/MyDrive/{mydir}\"\n",
        "!pwd\n",
        "     "
      ],
      "metadata": {
        "cellView": "form",
        "id": "yijC2e-9uR7O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ðŸŽ¯ To do  \n",
        "\n",
        "#@markdown   [1] Make a new folder: type a new folder name (e.g., myaudio)\n",
        "import os\n",
        "\n",
        "folder_name = input(\"Type a name for the new folder.\")\n",
        "\n",
        "if not os.path.exists(folder_name):\n",
        "  os.makedirs(folder_name)\n",
        "  print(f\"A new folder name '{folder_name}' has been created.\")\n",
        "else:\n",
        "  print(f\"{folder_name} already exists.\")\n",
        "\n",
        "#@markdown [2] Unzip files: type a zip file name (e.g., SE.zip)\n",
        "\n",
        "zipname = input(\"Type your zip file name (e.g., se.zip) to process (unzip and save them under the new folder\")\n",
        "!unzip \"/content/drive/MyDrive/asrdata/{zipname}\" -d \"/content/{folder_name}/\"\n",
        "\n",
        "print(f\"Your {zipname} is unzipped under '{folder_name}' folder\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "38H6TQTjulb7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ðŸŽ¯ Unmount Google drive (clearing): optional\n",
        "\n",
        "from google.colab import drive\n",
        "drive.flush_and_unmount()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "Jwq6gsDCuvNV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Filelist"
      ],
      "metadata": {
        "id": "_p_Vr3fAcHfY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "fname = []\n",
        "path = '/content/myaudio/'\n",
        "\n",
        "# use listdir() function to list files\n",
        "files = os.listdir(path)\n",
        "\n",
        "for file in files:\n",
        "  fname.append(file)\n",
        "\n",
        "filename = fname.sort()\n",
        "print(filename)\n"
      ],
      "metadata": {
        "id": "4dw3jANTcJAj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fname"
      ],
      "metadata": {
        "id": "bve1TLdvcsCD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_sorted = df.sort_values(by=\"Subject\")\n",
        "print(df_sorted)\n",
        "df_sorted['Filename'] = fname\n",
        "df_sorted"
      ],
      "metadata": {
        "id": "5KFOHcmidWHz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_sorted.to_csv('Engpro23PPRE_info.csv',index=False)"
      ],
      "metadata": {
        "id": "5TMAGDXIdlXA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# File from github: Engpro23PPRE_info.csv"
      ],
      "metadata": {
        "id": "QcENxCqtd8gT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://raw.githubusercontent.com/MK316/Engpro/main/data/Engpro23PPRE_info.csv\"\n",
        "df = pd.read_csv(url)\n",
        "df"
      ],
      "metadata": {
        "id": "2bv6tzCUd7kx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [2B] Openai"
      ],
      "metadata": {
        "id": "kr29nLY108Rw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai"
      ],
      "metadata": {
        "id": "Een-ZJo0u8gD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "openai.api_key = input('openai API key here')"
      ],
      "metadata": {
        "id": "4dUUb0-9u2PM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Reading audio from colab"
      ],
      "metadata": {
        "id": "tiVtwTci1WOI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ðŸŽ¯Install {autotime} to measure runtime (From here, runtime appears automatically.)\n",
        "%%capture\n",
        "!pip install ipython-autotime\n",
        "%load_ext autotime"
      ],
      "metadata": {
        "cellView": "form",
        "id": "d-etxSl04sZy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# A single file process to check\n",
        "# file = df['Filename'][0]\n",
        "audio_file = open('/content/myaudio/PF01.wav', \"rb\")\n",
        "transcript = openai.Audio.transcribe(\"whisper-1\", audio_file,language=\"en-US\")\n",
        "print(transcript['text'])"
      ],
      "metadata": {
        "id": "QbdTMG6J3GpX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir('/content/myaudio')\n",
        "\n",
        "recog = []\n",
        "\n",
        "for i in range(0, len(df['Subject'])):\n",
        "  file = df['Filename'][i]\n",
        "  audio_file = open(file, \"rb\")\n",
        "  transcript = openai.Audio.transcribe(\"whisper-1\", audio_file,language=\"en\")\n",
        "  recog.append(transcript['text'])\n",
        "print(recog)"
      ],
      "metadata": {
        "id": "1XXxOGsDvrie"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Recognized'] = recog\n",
        "df.tail()"
      ],
      "metadata": {
        "id": "UEm7Y7jO3v5w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Comparison between target and recognized\n",
        "se = df"
      ],
      "metadata": {
        "id": "2doFjvFy5HUM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Calculate recognition rate, record missing words => dataframe (df2) \n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "\n",
        "# number of words in the sentence\n",
        "nws = []\n",
        "# number of words in the recognized text\n",
        "nwr = []\n",
        "# number of missing words \n",
        "nmw = []\n",
        "# number of words actually recognized\n",
        "nwar = []\n",
        "\n",
        "# Recgonition Rate\n",
        "rr = []\n",
        "#\n",
        "nr = []\n",
        "# Missing word list\n",
        "mw = []\n",
        "# Correctly recognized word list\n",
        "recword=[]\n",
        "\n",
        "\n",
        "for i in range(0,len(se['Subject'])):\n",
        "  t1 = rainbow\n",
        "\n",
        "# text 1\n",
        "  txt1 = t1.lower()\n",
        "  tokenizer = RegexpTokenizer(r'\\w+')\n",
        "  wlist = tokenizer.tokenize(txt1)\n",
        "  nt = len(wlist)\n",
        "  nws.append(nt)\n",
        "\n",
        "# text 2\n",
        "  t2 = se['Recognized'][i]\n",
        "  txt2 = t2.lower()\n",
        "  tokenizer = RegexpTokenizer(r'\\w+')\n",
        "  wlist1 = tokenizer.tokenize(txt2)\n",
        "  nt1 = len(wlist1)\n",
        "  nwr.append(nt1)\n",
        "\n",
        "# Recognition rate\n",
        "\n",
        "  # from tables.idxutils import calc_chunksize\n",
        "  # from nltk.downloader import ErrorMessage\n",
        "# txt1(original text), txt2 (recognized text)\n",
        " \n",
        "  mword = []\n",
        "  rword = []\n",
        "  score = 0\n",
        "  for i in range(0, len(wlist1)):\n",
        "      w = wlist1[i]\n",
        "\n",
        "      if w in wlist:\n",
        "        sc = 1\n",
        "        rword.append(w)\n",
        "      else:\n",
        "        sc = 0\n",
        "        mword.append(w)\n",
        "\n",
        "      score = score + sc\n",
        "      mwords = ', '.join(mword)\n",
        "      rwords = ', '.join(rword)\n",
        "  mw.append(mwords)\n",
        "  missingword = round((score/len(wlist))*100,2)\n",
        "  nr.append(score)\n",
        "  recword.append(rwords)\n",
        "\n",
        "  # RecRate = float(format(missingword, '.2f'))\n",
        "  # ErrRate = float(format((100.0 - RecRate), '.2f'))\n",
        "  \n",
        "  rr.append(missingword)\n",
        "  # print('Matching words: %d'%score, 'out of %d words'%len(wlist))\n",
        "  # print(\"=\"*50)\n",
        "  # print('Recognition Rate: %f %%'%RecRate)\n",
        "  # print('Error Rate: %f %%'%ErrRate)\n",
        "\n",
        "se['LenS'] = nws\n",
        "se['LenR'] = nwr\n",
        "se['N_RecW'] = nr\n",
        "se['RecRate'] = rr\n",
        "se['Recognized_words'] = recword\n",
        "se['MissRec_words'] = mw\n",
        "\n",
        "se.head(); se.tail()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "6vtdPOKE4jBw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "se.describe()"
      ],
      "metadata": {
        "id": "MoCHLPzD574S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1 = se\n",
        "df1.to_csv(\"EngproPPRE_recognized.csv\", index=False)"
      ],
      "metadata": {
        "id": "eAQdApun6Fwa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ’œ From here: df (with recognized words and texts)"
      ],
      "metadata": {
        "id": "CC6DnnyN6XfC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/MK316/Engpro/main/data/EngproPPRE_recognized.csv\"\n",
        "df = pd.read_csv(url)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "Z-jzhT7r6trC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data = df[['Filename', 'N_RecW','RecRate']]\n",
        "target = []\n",
        "\n",
        "for i in range(0, len(df['Filename'])):\n",
        "  target.append(rainbow)\n",
        "df['Target']=target\n",
        "df.tail()"
      ],
      "metadata": {
        "id": "boqxu9-Z7R9R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# WER"
      ],
      "metadata": {
        "id": "_kDgoAul8f0L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown + Install and import libraries\n",
        "%%capture\n",
        "!pip install Levenshtein\n",
        "import Levenshtein as lev\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "B4QhHVhn8IMc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown + Calculate WER and WER_lev and add columns to df\n",
        "# import numpy as np\n",
        "# import pandas as pd\n",
        "# import Levenshtein as lev\n",
        "#@markdown + Column names 'Original' and 'Recognized' for comparison\n",
        "def calculate_wer_word_level(s1, s2):\n",
        "    \"\"\"\n",
        "    Calculation of WER with Dynamic Time Warping algorithm at word level.\n",
        "    \"\"\"\n",
        "\n",
        "    # splitting the sentences into words\n",
        "    s1_words = s1.split()\n",
        "    s2_words = s2.split()\n",
        "\n",
        "    # preparing the matrix for the Dynamic Time Warping algorithm\n",
        "    dtw_matrix = np.zeros((len(s1_words)+1, len(s2_words)+1))\n",
        "    for i in range(len(s1_words)+1):\n",
        "        for j in range(len(s2_words)+1):\n",
        "            if i == 0:\n",
        "                dtw_matrix[0][j] = j\n",
        "            elif j == 0:\n",
        "                dtw_matrix[i][0] = i\n",
        "\n",
        "    # Dynamic Time Warping algorithm\n",
        "    for i in range(1, len(s1_words)+1):\n",
        "        for j in range(1, len(s2_words)+1):\n",
        "            cost = 0 if s1_words[i-1] == s2_words[j-1] else 1\n",
        "            dtw_matrix[i][j] = cost + min([dtw_matrix[i-1][j], dtw_matrix[i][j-1], dtw_matrix[i-1][j-1]])\n",
        "\n",
        "    wer = dtw_matrix[len(s1_words)][len(s2_words)] / len(s1_words)\n",
        "\n",
        "    return wer\n",
        "\n",
        "def calculate_wer_lev(s1, s2):\n",
        "    \"\"\"\n",
        "    Calculation of WER with Levenshtein distance at character level.\n",
        "    \"\"\"\n",
        "\n",
        "    # splitting the sentences into words\n",
        "    s1_words = s1.split()\n",
        "    s2_words = s2.split()\n",
        "\n",
        "    # calculating Levenshtein distance\n",
        "    edit_distance = lev.distance(' '.join(s1_words), ' '.join(s2_words))\n",
        "\n",
        "    # normalizing by the length of the original sentence (s1)\n",
        "    wer = edit_distance / len(s1_words)\n",
        "\n",
        "    return wer\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "5HHOYQA48iTM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming df is your DataFrame and it has columns 'Sentence' and 'Recognized'\n",
        "import numpy as np\n",
        "\n",
        "wer_values_word_level = df.apply(lambda row: calculate_wer_word_level(row['Target'], row['Recognized']), axis=1)\n",
        "wer_values_lev = df.apply(lambda row: calculate_wer_lev(row['Target'], row['Recognized']), axis=1)\n",
        "\n",
        "# Adding WER values as new columns to the DataFrame\n",
        "df['WER'] = wer_values_word_level\n",
        "df['WER_lev'] = wer_values_lev"
      ],
      "metadata": {
        "id": "p_-JapQ789UO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv('Engpro23PPRE_result.csv')"
      ],
      "metadata": {
        "id": "RD_0jTOj9MsT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Result preview"
      ],
      "metadata": {
        "id": "2pITkgN1rEn1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir('/content')"
      ],
      "metadata": {
        "id": "FgndQO6T-mrN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://raw.githubusercontent.com/MK316/Engpro/main/data/Engpro23PPRE_result.csv\"\n",
        "df = pd.read_csv(url)\n",
        "df.describe()\n"
      ],
      "metadata": {
        "id": "6ohLF3tH-TQU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "MissRec_words"
      ],
      "metadata": {
        "id": "17L6QljKsB3w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_missing = []\n",
        "\n",
        "for i in range(0,len(df['Filename'])):\n",
        "  wlist = df['MissRec_words'][i]\n",
        "  w = wlist.split(',')\n",
        "  n = len(w)\n",
        "  n_missing.append(n)\n",
        "\n",
        "df['N_missing'] = n_missing\n",
        "df.describe()"
      ],
      "metadata": {
        "id": "ILvIZ0HisBSD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1) Regression: Missing words ~ WER"
      ],
      "metadata": {
        "id": "JH1Vn3IIAfMg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import statsmodels.formula.api as smf\n",
        "\n",
        "# Assuming df is your dataframe and it has columns 'LOR' and 'WER'\n",
        "# Create a fitted model\n",
        "model = smf.ols(formula='WER ~ N_missing', data=df).fit()\n",
        "\n",
        "# Print out the statistics of the model\n",
        "print(model.summary())\n"
      ],
      "metadata": {
        "id": "SPK81qJYAgeQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's break down the results:\n",
        "\n",
        "R-squared: This is the coefficient of determination. It explains the proportion of the variance in the dependent variable that is predictable from the independent variable. In this case, 15.3% of the variability in WER can be explained by LOR. This isn't particularly high, which might suggest that other variables not included in the model could also be impacting WER.\n",
        "\n",
        "Adj. R-squared: This is the R-squared value that has been adjusted based on the number of predictors in the model. It is more accurate and should be used over the regular R-squared if you have more than one predictor. Since this model has only one predictor (LOR), the adjusted R-squared is very close to the regular R-squared.\n",
        "\n",
        "coef: These are the coefficients of the linear regression equation. The equation would look something like this: WER = 0.0473 + 0.0072*LOR. That means, for each unit increase in LOR, WER increases by 0.0072 units, on average, assuming all other variables are held constant.\n",
        "\n",
        "P>|t|: This is the p-value associated with the null hypothesis that the coefficient is equal to zero (no effect). A p-value of less than 0.05 is typically considered statistically significant at the 5% level. In this case, the p-value for LOR is 0.020, which is less than 0.05, suggesting that LOR has a statistically significant effect on WER.\n",
        "\n",
        "[0.025 0.975]: These are the 95% confidence intervals for the coefficients. This means we are 95% confident that the true population coefficient for LOR lies between 0.001 and 0.013.\n",
        "\n",
        "Durbin-Watson: This tests for the presence of autocorrelation (a relationship between values separated from each other by a given time lag) in the residuals. Values range from 0 to 4, and values around 2 suggest no autocorrelation. Your value of 1.890 suggests that there is no significant autocorrelation.\n",
        "\n",
        "Prob (F-statistic): This is the p-value of the overall regression model. It tests the null hypothesis that all regression coefficients are equal to zero. A p-value of less than 0.05 indicates that at least one of the predictors is significantly related to the outcome variable. In this case, the p-value is 0.0201, which is less than 0.05, indicating that the model is statistically significant.\n",
        "\n",
        "Finally, it's important to remember that correlation does not imply causation. While LOR might be associated with WER, it doesn't necessarily mean that changes in LOR cause changes in WER."
      ],
      "metadata": {
        "id": "2lB1UofiCC2a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Regression line plot\n",
        "sns.regplot(x='N_missing', y='WER', data=df)\n",
        "plt.show()\n",
        "\n",
        "# Residuals plot\n",
        "residuals = df['WER'] - model.predict()\n",
        "sns.residplot(x=model.predict(), y=residuals, lowess=True, color=\"g\")\n",
        "plt.show()\n",
        "\n",
        "# QQ plot\n",
        "import statsmodels.api as sm\n",
        "sm.qqplot(residuals, line ='45')\n",
        "plt.show()\n",
        "\n",
        "# Histogram of residuals\n",
        "plt.hist(residuals)\n",
        "plt.show()\n",
        "\n",
        "# Correlation matrix\n",
        "corrMatrix = df.corr()\n",
        "sns.heatmap(corrMatrix, annot=True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "nz9wIXACAhJv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are a few ways to visually summarize the results of your regression analysis:\n",
        "\n",
        "Scatter plot with fitted line (Regression line plot): You can create a scatter plot of the data and add the line of best fit from your model. This will provide a visualization of how well the line fits the data. You can also plot the residuals (the differences between the observed and predicted values) to see how evenly they're distributed around the line.\n",
        "\n",
        "Residuals plot: A residuals plot shows the difference between the observed and predicted values (the residuals) for each observation, typically plotted against the predicted values. If your model is a good fit, the points in the residuals plot should be randomly and evenly dispersed around zero.\n",
        "\n",
        "QQ Plot: A Quantile-Quantile plot, or QQ plot, is used to check if your data follows a particular distribution. In the case of linear regression, we often want to check if the residuals follow a normal distribution. If the points roughly fall along the diagonal line, then the residuals are normally distributed.\n",
        "\n",
        "Histogram of residuals: This can also be used to check if residuals are normally distributed.\n",
        "\n",
        "Correlation matrix: While not directly related to the regression results, a correlation matrix can give you an overview of how all your variables relate to each other.\n",
        "\n",
        "Here is how you can plot some of these in Python:"
      ],
      "metadata": {
        "id": "fyWUWpB1CHZ9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2) Boxplot"
      ],
      "metadata": {
        "id": "wOpnsGHWt6iJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df1['Category'] = pd.qcut(df1['WER'], 4, labels=['A', 'B', 'C', 'D'])\n",
        "df1.head()\n",
        "\n",
        "# df1.to_csv('Engpro23PPRE_results_total.csv', index=False)"
      ],
      "metadata": {
        "id": "7YEkHDhCyvyR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bins = [0, 0.1, 0.2, 0.3, float('inf')]\n",
        "labels = ['A', 'B', 'C', 'D']\n",
        "\n",
        "df1['WER_cat'] = pd.cut(df1['WER'], bins=bins, labels=labels)\n"
      ],
      "metadata": {
        "id": "jV2gWRQb4OM_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1.to_csv('EngproPPRE_resultstotal.csv', index=False)"
      ],
      "metadata": {
        "id": "hLPTPKK04vKA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create a boxplot\n",
        "sns.boxplot(data=df[['N_missing']])\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "yucxHjUat8YN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Excluding outlier"
      ],
      "metadata": {
        "id": "OfUYJQt1ulJ_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df1 = df[df['N_missing'] <= 30]"
      ],
      "metadata": {
        "id": "PYDdUrlkukpB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create a boxplot\n",
        "sns.boxplot(data=df1[['WER']])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "mJmwb6MzupMR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create a figure and a set of subplots\n",
        "fig, axs = plt.subplots(1, 2, figsize=(10,5))\n",
        "\n",
        "# First boxplot\n",
        "axs[0].boxplot(df1['N_missing'])\n",
        "axs[0].set_title('Boxplot of Number of missing owrds')\n",
        "axs[0].set_ylim([0,25])  # Set y limits for first plot\n",
        "\n",
        "# Second boxplot\n",
        "axs[1].boxplot(df1['WER'])\n",
        "axs[1].set_title('Boxplot of WER')\n",
        "axs[1].set_ylim([0,0.25])  # Set y limits for second plot\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "E_LliGfSxOQK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary = df1.groupby(['Category', 'WER_cat']).size().reset_index(name='Count')\n",
        "print(summary)"
      ],
      "metadata": {
        "id": "btbSv-SO5o98"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pivot_table = df1.pivot_table(index='Category', columns='WER_cat', aggfunc='size', fill_value=0)\n",
        "print(pivot_table)\n"
      ],
      "metadata": {
        "id": "daFKMBrA6UeR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.boxplot(x='Category', y='WER', data=df1)\n",
        "\n",
        "plt.title('Boxplot of WER by Category')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lfntzRm35czX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.boxplot(x='WER_cat', y='WER', data=df1)\n",
        "\n",
        "plt.title('Boxplot of WER by Category')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "rRCOG5nqyWNR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Different scales: N_missing and WER"
      ],
      "metadata": {
        "id": "O4ojKpMOwWfG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, ax1 = plt.subplots()\n",
        "\n",
        "# Plot the first data series in blue on the left y-axis\n",
        "ax1.set_ylabel('First Data Series', color='b')\n",
        "ax1.plot(df['N_missing'], color='b')\n",
        "ax1.tick_params(axis='y', labelcolor='b')\n",
        "\n",
        "# Then create a second y-axis for the second data series\n",
        "ax2 = ax1.twinx()\n",
        "\n",
        "# Plot the second data series in red on the right y-axis\n",
        "ax2.set_ylabel('Second Data Series', color='r')\n",
        "ax2.plot(df['WER'], color='r')\n",
        "ax2.tick_params(axis='y', labelcolor='r')\n",
        "\n",
        "fig.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "vFoBE24hwEJr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gDQg5u8vvpSF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pRgbs8VBwl_z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}